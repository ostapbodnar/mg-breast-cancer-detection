{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7u7vqPx6jzzA",
    "outputId": "2fac9ca0-ff59-4112-daec-78cc0ad45188"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:29.452017Z",
     "start_time": "2023-10-21T18:42:29.394424Z"
    },
    "id": "WciSTxNvjtLP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "data = torch.load(\"gdrive/MyDrive/preprocessed_images_with_metadata_and_target.pt.nosync\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def flip_image(image):\n",
    "  left_side_count = np.count_nonzero(image[:, :image.shape[1] // 2])\n",
    "  right_side_count = np.count_nonzero(image[:, image.shape[1] // 2:])\n",
    "\n",
    "  if left_side_count < right_side_count:\n",
    "      flipped_image = np.fliplr(image)\n",
    "  else:\n",
    "      flipped_image = image\n",
    "  return flipped_image"
   ],
   "metadata": {
    "id": "-8nxIXQzHS2i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = []\n",
    "metadata = []\n",
    "y = []\n",
    "\n",
    "perm = torch.randperm(len(data))\n",
    "idx = perm[:700]\n",
    "\n",
    "for id in idx:\n",
    "    [target, age, implant], image = data[id]\n",
    "    features.append(flip_image(image.numpy()))\n",
    "    metadata.append((age, implant))\n",
    "    y.append(target)\n",
    "del data, idx, perm"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:30.011391Z",
     "start_time": "2023-10-21T18:42:29.994266Z"
    },
    "id": "-nZCFAQcjtLS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "f_np = np.array(features).reshape((-1, 1, 512, 512))\n",
    "features = torch.tensor(f_np).float()\n",
    "del f_np\n",
    "m_np = np.asarray(metadata).reshape((-1, 2))\n",
    "metadata = torch.tensor(m_np).float()\n",
    "del m_np\n",
    "y = torch.tensor(y).view(-1, 1).float()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:31.186926Z",
     "start_time": "2023-10-21T18:42:31.044364Z"
    },
    "id": "3l3kFLRPjtLS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0afffe00-bd5c-4489-a805-ad331cf920e3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([700, 1, 512, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "features.shape"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:43.106057Z",
     "start_time": "2023-10-21T18:42:43.097788Z"
    },
    "id": "jncTRvt-jtLT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "648eeab8-acdd-4931-be9c-9c722e558a49"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([700, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "metadata.shape"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:32.548218Z",
     "start_time": "2023-10-21T18:42:32.541900Z"
    },
    "id": "7ptKdJ7KjtLU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5aa5567b-7648-4b77-948f-770508a6a593"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(features, metadata, y, test_size=0.15,\n",
    "                                                                      random_state=3451)\n",
    "\n",
    "train_dataset = TensorDataset(X1_train, X2_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "val_dataset = TensorDataset(X1_val, X2_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:51.099480Z",
     "start_time": "2023-10-21T18:42:51.077152Z"
    },
    "id": "6LH1a5LejtLV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def probabilistic_f1(labels, predictions, beta=1):\n",
    "    pTP = np.sum(labels * predictions)\n",
    "    pFP = np.sum((1 - labels) * predictions)\n",
    "    num_positives = np.sum(labels)  #  = pTP+pFN\n",
    "\n",
    "    pPrecision = pTP / (pTP + pFP)\n",
    "    pRecall = pTP / num_positives\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "\n",
    "    if pPrecision > 0 and pRecall > 0:\n",
    "        pF1 = (1 + beta_squared) * pPrecision * pRecall / (beta_squared * pPrecision + pRecall)\n",
    "        return pF1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T18:44:27.932716Z",
     "start_time": "2023-10-21T18:44:27.928890Z"
    },
    "id": "gUHUdakxjtLW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc1 = self.conv_block(in_channels, 32)\n",
    "        self.enc2 = self.conv_block(32, 64)\n",
    "        self.enc3 = self.conv_block(64, 128)\n",
    "        self.enc4 = self.conv_block(128, 1024)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=9, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=6, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        return enc4\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.encoder = Encoder(1)\n",
    "        self.avgpool = nn.AdaptiveMaxPool2d(output_size=1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_output = self.encoder(x)\n",
    "        avg_pooled = self.avgpool(enc_output)\n",
    "        avg_pooled = avg_pooled.view(avg_pooled.size(0), -1)\n",
    "        output = self.classifier(avg_pooled)\n",
    "        return output"
   ],
   "metadata": {
    "id": "xN2dWMkrjtLZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = CustomModel().to('cuda')"
   ],
   "metadata": {
    "id": "I0Ey79bd6-KY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model"
   ],
   "metadata": {
    "id": "Vq2Q3C1E7tvW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3cb97c7f-2e94-4af2-a6b3-c0b60d57e324"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (encoder): Encoder(\n",
       "    (enc1): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(9, 9), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(32, 32, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (enc2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(9, 9), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (enc3): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(9, 9), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (enc4): Sequential(\n",
       "      (0): Conv2d(128, 1024, kernel_size=(9, 9), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveMaxPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import optim\n",
    "\n",
    "model = CustomModel().to('cuda')\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 400\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "best_val_score = float('-inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    plt.clf()\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs1, _, labels = data\n",
    "        inputs1, labels = inputs1.to('cuda'),  labels.to('cuda')\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs1*255)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_predictions.extend(outputs.detach().cpu().numpy().flatten())\n",
    "        train_labels.extend(labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / (i + 1)\n",
    "    train_losses.append(train_loss)\n",
    "    train_f1 = probabilistic_f1(np.asarray(train_labels), np.round(train_predictions, decimals=2))\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            inputs1, _, labels = data\n",
    "            inputs1, labels = inputs1.to('cuda'),  labels.to('cuda')\n",
    "\n",
    "            outputs = model(inputs1*255)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_predictions.extend(outputs.detach().cpu().numpy().flatten())\n",
    "            val_labels.extend(labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / (i + 1)\n",
    "    val_losses.append(val_loss)\n",
    "    val_f1 = probabilistic_f1(np.asarray(val_labels), np.asarray(val_predictions))\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, Validation Loss: {round(val_loss, 3)}, F1 Score: {round(val_f1, 3)} Training Loss: {round(train_loss, 3)}, F1 Score: {round(train_f1, 3)}')\n",
    "    if val_f1 > best_val_score:\n",
    "      print(\"Saving model\")\n",
    "      best_val_score = val_f1\n",
    "      torch.save(model.state_dict(), \"gdrive/MyDrive/cnn_mg.pt\")\n",
    "\n"
   ],
   "metadata": {
    "id": "tu5AnA1I1dJW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "outputId": "c21d350f-5a02-4135-ba4e-a568fbd98db9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1, Validation Loss: 0.688, F1 Score: 0.517 Training Loss: 0.705, F1 Score: 0.513\n",
      "Saving model\n",
      "Epoch 2, Validation Loss: 0.669, F1 Score: 0.526 Training Loss: 0.692, F1 Score: 0.521\n",
      "Saving model\n",
      "Epoch 3, Validation Loss: 0.689, F1 Score: 0.486 Training Loss: 0.689, F1 Score: 0.524\n",
      "Epoch 4, Validation Loss: 0.671, F1 Score: 0.534 Training Loss: 0.687, F1 Score: 0.529\n",
      "Saving model\n",
      "Epoch 5, Validation Loss: 0.685, F1 Score: 0.503 Training Loss: 0.679, F1 Score: 0.532\n",
      "Epoch 6, Validation Loss: 0.68, F1 Score: 0.529 Training Loss: 0.669, F1 Score: 0.535\n",
      "Epoch 7, Validation Loss: 0.695, F1 Score: 0.487 Training Loss: 0.664, F1 Score: 0.543\n",
      "Epoch 8, Validation Loss: 0.703, F1 Score: 0.514 Training Loss: 0.659, F1 Score: 0.543\n",
      "Epoch 9, Validation Loss: 0.737, F1 Score: 0.556 Training Loss: 0.656, F1 Score: 0.549\n",
      "Saving model\n",
      "Epoch 10, Validation Loss: 0.674, F1 Score: 0.542 Training Loss: 0.669, F1 Score: 0.555\n",
      "Epoch 11, Validation Loss: 0.696, F1 Score: 0.526 Training Loss: 0.656, F1 Score: 0.544\n",
      "Epoch 12, Validation Loss: 0.692, F1 Score: 0.534 Training Loss: 0.65, F1 Score: 0.554\n",
      "Epoch 13, Validation Loss: 0.71, F1 Score: 0.492 Training Loss: 0.653, F1 Score: 0.553\n",
      "Epoch 14, Validation Loss: 0.716, F1 Score: 0.513 Training Loss: 0.645, F1 Score: 0.559\n",
      "Epoch 15, Validation Loss: 0.71, F1 Score: 0.497 Training Loss: 0.63, F1 Score: 0.567\n",
      "Epoch 16, Validation Loss: 0.76, F1 Score: 0.435 Training Loss: 0.631, F1 Score: 0.57\n",
      "Epoch 17, Validation Loss: 0.693, F1 Score: 0.51 Training Loss: 0.638, F1 Score: 0.561\n",
      "Epoch 18, Validation Loss: 0.787, F1 Score: 0.428 Training Loss: 0.626, F1 Score: 0.58\n",
      "Epoch 19, Validation Loss: 0.762, F1 Score: 0.549 Training Loss: 0.632, F1 Score: 0.573\n",
      "Epoch 20, Validation Loss: 0.729, F1 Score: 0.495 Training Loss: 0.648, F1 Score: 0.56\n",
      "Epoch 21, Validation Loss: 0.778, F1 Score: 0.478 Training Loss: 0.61, F1 Score: 0.59\n",
      "Epoch 22, Validation Loss: 0.759, F1 Score: 0.542 Training Loss: 0.599, F1 Score: 0.592\n",
      "Epoch 23, Validation Loss: 0.782, F1 Score: 0.521 Training Loss: 0.633, F1 Score: 0.581\n",
      "Epoch 24, Validation Loss: 0.877, F1 Score: 0.491 Training Loss: 0.576, F1 Score: 0.611\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-38-b0080e8b08d4>\u001B[0m in \u001B[0;36m<cell line: 18>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m         \u001B[0mtrain_predictions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0mtrain_labels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "2hMRsLGZHBd2"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ml-nulp-masters-degree",
   "language": "python",
   "display_name": "ml-nulp-masters-degree"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
