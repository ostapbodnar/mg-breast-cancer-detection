{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:29.452017Z",
     "start_time": "2023-10-21T18:42:29.394424Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "data = torch.load(\"preprocessed_images_with_metadata_and_target.pt.nosync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "features = []\n",
    "metadata = []\n",
    "y = []\n",
    "\n",
    "for [target, age, implant], image in data:\n",
    "    features.append(image.numpy())\n",
    "    metadata.append((age, implant))\n",
    "    y.append(target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:30.011391Z",
     "start_time": "2023-10-21T18:42:29.994266Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "features = torch.tensor(np.array(features).reshape((-1, 1, 512, 512))).float()\n",
    "metadata = torch.tensor(np.asarray(metadata).reshape((-1, 2))).float()\n",
    "y = torch.tensor(y).view(-1, 1).float()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:31.186926Z",
     "start_time": "2023-10-21T18:42:31.044364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([50, 1, 512, 512])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:43.106057Z",
     "start_time": "2023-10-21T18:42:43.097788Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([50, 2])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:32.548218Z",
     "start_time": "2023-10-21T18:42:32.541900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:39:13.662559Z",
     "start_time": "2023-10-21T18:39:13.657220Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(features, metadata, y, test_size=0.15,\n",
    "                                                                      random_state=3451)\n",
    "\n",
    "train_dataset = TensorDataset(torch.Tensor(np.asarray(X1_train)).float(), X2_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.Tensor(np.asarray(X1_val)).float(), X2_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:42:51.099480Z",
     "start_time": "2023-10-21T18:42:51.077152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from torchvision.models import EfficientNet_B2_Weights\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EfficientNetGrayWithoutClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetGrayWithoutClassifier, self).__init__()\n",
    "        self.base_model = models.efficientnet_b2(weights=EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Linear(1408, 512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.seq_2 = nn.Sequential(\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.base_model.eval()\n",
    "\n",
    "    def forward(self, single_channel_input, metadata):\n",
    "        rgb_input = torch.cat([single_channel_input] * 3, dim=1)\n",
    "        out_1 = self.base_model(rgb_input)\n",
    "        out_2 = self.seq_2(metadata)\n",
    "\n",
    "        out = torch.cat([out_1, out_2], dim=1)\n",
    "        out = self.linear(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:44:27.210229Z",
     "start_time": "2023-10-21T18:44:27.202669Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def probabilistic_f1(labels, predictions, beta=1):\n",
    "    pTP = np.sum(labels * predictions)\n",
    "    pFP = np.sum((1 - labels) * predictions)\n",
    "    num_positives = np.sum(labels)  #  = pTP+pFN\n",
    "\n",
    "    pPrecision = pTP / (pTP + pFP)\n",
    "    pRecall = pTP / num_positives\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "\n",
    "    if pPrecision > 0 and pRecall > 0:\n",
    "        pF1 = (1 + beta_squared) * pPrecision * pRecall / (beta_squared * pPrecision + pRecall)\n",
    "        return pF1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:44:27.932716Z",
     "start_time": "2023-10-21T18:44:27.928890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "results = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:44:28.229969Z",
     "start_time": "2023-10-21T18:44:28.224217Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.693, F1 Score: 0.496 Training Loss: 0.694, F1 Score: 0.517\n",
      "Epoch 2, Validation Loss: 0.693, F1 Score: 0.496 Training Loss: 0.694, F1 Score: 0.517\n",
      "Epoch 3, Validation Loss: 0.693, F1 Score: 0.497 Training Loss: 0.693, F1 Score: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import optim\n",
    "\n",
    "model = EfficientNetGrayWithoutClassifier()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    plt.clf()\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs1, inputs2, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs1, inputs2)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        train_predictions.extend(outputs.detach().numpy().flatten())\n",
    "        train_labels.extend(labels.detach().numpy().flatten())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / (i + 1)\n",
    "    train_losses.append(train_loss)\n",
    "    train_f1 = probabilistic_f1(np.asarray(train_labels), np.round(train_predictions, decimals=2))\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            inputs1, inputs2, labels = data\n",
    "\n",
    "            outputs = model(inputs1, inputs2)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_predictions.extend(outputs.detach().numpy().flatten())\n",
    "            val_labels.extend(labels.detach().numpy().flatten())\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / (i + 1)\n",
    "    val_losses.append(val_loss)\n",
    "    val_f1 = probabilistic_f1(np.asarray(val_labels), np.asarray(val_predictions))\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, Validation Loss: {round(val_loss, 3)}, F1 Score: {round(val_f1, 3)} Training Loss: {round(train_loss, 3)}, F1 Score: {round(train_f1, 3)}')\n",
    "\n",
    "# Real-time visualization\n",
    "plt.figure(1)\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(train_f1_scores, label='Training F1 Score', color='green')\n",
    "plt.plot(val_f1_scores, label='Validation F1 Score', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Training and Validation F1 Score')\n",
    "\n",
    "plt.show(block=False)\n",
    "\n",
    "y_pred = np.asarray(val_predictions) > 0.5\n",
    "accuracy = accuracy_score(val_labels, y_pred)\n",
    "precision = precision_score(val_labels, y_pred, average='weighted')\n",
    "recall = recall_score(val_labels, y_pred, average='weighted')\n",
    "auroc = roc_auc_score(val_labels, val_predictions, multi_class='ovr')\n",
    "\n",
    "clf_name = \"DeepNN - val\"\n",
    "results[clf_name] = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': val_f1,\n",
    "    'AUROC': auroc,\n",
    "}\n",
    "y_pred = np.asarray(train_predictions) > 0.5\n",
    "accuracy = accuracy_score(train_labels, y_pred)\n",
    "precision = precision_score(train_labels, y_pred, average='weighted')\n",
    "recall = recall_score(train_labels, y_pred, average='weighted')\n",
    "auroc = roc_auc_score(train_labels, train_predictions, multi_class='ovr')\n",
    "\n",
    "clf_name = \"DeepNN - train\"\n",
    "results[clf_name] = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': train_f1,\n",
    "    'AUROC': auroc,\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:49:11.313314Z",
     "start_time": "2023-10-21T18:44:28.748146Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ml-nulp-masters-degree",
   "language": "python",
   "display_name": "ml-nulp-masters-degree"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
